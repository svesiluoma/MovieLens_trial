---
title: "Movielens Project"
author: "Sari Vesiluoma"
date: "1 11 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r create_d, include=FALSE}
library(tidyverse)
library(caret)
library(dslabs)

################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

### End of the given code ###
```

## Introduction

The purpose of this project is to introduce a movie recommender based on the Movielens 10M data. The data preparation was done based on the code given in the exercice introduction. Ratings and movies were separately excerpted from the Movielens data, then combined. Then movielens data was split into two groups, one data frame called edx for training data and one data frame called validation for validation data including 10 % of the movielens data. Validation data was created so, that it included only rows where movieId and userId were also existing in the training set (edx). Rows which were excluded from the validation set were added back to the training set. 

The dimensions of the training data (edx) and validation data are

```{r dims}
dim(edx)
dim(validation)
```

Like will be shown in the Analysis chapter, the data did not need any further cleanig, so it was possible to start creating the recommendation model, a.k.a. starting to find a solution resulting reasonably small RMSE. This was done in three steps. First to assume a model where the recommendation is the mean over all ratings in the training set, secondly to notice also that different movies get different level of ratings, and finally, in addition, noticing that different users give different level of ratings for different movies. 


## Analysis

The summary of the training set clearly shows that there are no NA values, so no need for pre-processing those in this data. So, this data can be used as is. 

```{r summary}
summary(edx)
```

Rows in the data look like this

```{r head}
edx %>% head()
```

The amount of unique movies and users in the training data are

```{r amounts}
n_distinct(edx$movieId)
n_distinct(edx$userId)
```

When using this data, it is important to recognize that the amount of ratings per movie vary hugely between the movies as shown in the histogram below.

```{r ratings_movie, echo=FALSE}
ratings <- edx %>% group_by(movieId, title) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
hist(ratings$count)
```

Similarly, the amount of ratings per user vary really much as can be seen the next histogram.

```{r ratings_user, echo=FALSE}
ratings_per_user <- edx %>% group_by(userId) %>%
  summarize(count_user = n()) %>%
  arrange(desc(count_user))
hist(ratings_per_user$count_user)
```

Also, when looking the way people have used rating levels, it is clear that they have used integer ratings much more than the half (.5) ratings as can bee seen from the line chart below. 

```{r ratings_rates, echo=FALSE}
edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(rating, count)) +
  geom_line()
```

The idea here is to use several different machine learning algorithms to test which one(s) might give the best result in recommending. The value of the result will be analyzed based on RMSE score of the algorithm. 

```{r rmse, results="hidden"}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings-predicted_ratings)^2))
}
```

To start with we will need a some kind of a baseline compared to which to try to improve the RMSE. For that purpose we will use the mean value of the ratings in the training set. The resulting RMSE value is

```{r average, echo=FALSE}
mu_hat <- mean(edx$rating)
naive_rmse <- RMSE(validation$rating, mu_hat)
naive_rmse
rmse_results <- data.frame(method = "Based on average", RMSE=naive_rmse)
```

The first model to be used is to notice that some movies get higher rates than others. So, in addition to using the average, we'll notice movie specific deviation from the mean. When noticing this the value of RMSE is less than when using only the average value and it is:

```{r movie_effect, echo=FALSE}
# Differences in ratings per movie
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu_hat))
# Taking into consideration that some movies are rated higher than the others
predicted_ratings <- mu_hat + validation %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
model_1_rmse
# Adding the resulting value to the list
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Movie effect model", 
                                     RMSE = model_1_rmse))
```

Like some movies get higher rates than others, similarly different users rate very differently. Next, in addition to using the average and movie effect, we will also use the user effect. Like you see, the resulting RMSE improves to

```{r movie_user_effect, echo=FALSE}

# Differences in ratings per user and noticing both movie effect and user effect
user_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_hat - b_i))
# Taking into consideration both movie and user effect
predicted_ratings <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu_hat + b_i + b_u) %>%
  pull(pred)
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
model_2_rmse
# Adding the resulting value to the list
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Movie + user effect model", 
                                     RMSE = model_2_rmse))
```

## Results

The results were achived following the model that had been introduced in the lectures. The results based on the three models used are:

```{r results}
rmse_results
```

The comparing value was created based on using the rating average only. The next value was created based on noticing that different movies are rated differently. That clearly improved the result. Finally, also it was noticed that different users rate differently, so including also the user effect in addition resulted RMSE being 'model_2_rmse'.


## Conclusion

When recommending movies, it is clear, also based on this project, that in addition to an average as the basis for a recommendation, we will need to notice also that different movies are rated differently (movie effect) and that different users give rates differently (user effect). Includin both of these, the movie effect and the user effect increases the "goodness" of the recommendation remarkably. 

The main limitation in this project was that only a small amount of data (10 M Movielens data) was used not to get the home laptops stuck. So, to be able to create even more convincing recommendations, more training data would be beneficial. In addition, some other algorithms could also be utilized in addition to this simple approach used here. 

As such, this was an interesting project and made me understand much better what we were doing during the lectures and why to do these this way. 
